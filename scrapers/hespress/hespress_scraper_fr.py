import time
import requests
from bs4 import BeautifulSoup
import sys
import os

# Ajouter le r√©pertoire racine du projet au chemin d'acc√®s pour permettre les imports personnalis√©s
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))
from config.mongodb import get_mongo_collection  # Import de la fonction pour r√©cup√©rer la collection MongoDB


def extract_urls_from_sitemap(sitemap_url):
    """
    Extrait les URLs des articles √† partir d'un fichier sitemap XML.
    
    :param sitemap_url: URL du sitemap √† analyser
    :return: Liste des URLs d'articles extraites
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    try:
        response = requests.get(sitemap_url, headers=headers, timeout=10)
        time.sleep(3)  # Pause pour √©viter d'√™tre bloqu√© par le serveur

        if response.status_code != 200:
            print(f"‚ùå Erreur {response.status_code} pour {sitemap_url}")
            return []

        soup = BeautifulSoup(response.content, "xml")  # Parser XML pour extraire les URLs
        print(f"R√©cup√©ration du sitemap : {sitemap_url}")

        urls = [loc.text for loc in soup.find_all("loc")]  # Extraction des URLs

        # Filtrer pour ne garder que les URLs .html (√©viter les images .jpg et .jpeg)
        filtered_urls = [url for url in urls if not (url.endswith(".jpg") or url.endswith(".jpeg"))]

        return filtered_urls
    
    except Exception as e:
        print(f"‚ùå Erreur lors de l'extraction du sitemap {sitemap_url}: {e}")
        return []

# URL du sitemap principal
sitemap_index = "https://fr.hespress.com/sitemap.xml"
sub_sitemaps = extract_urls_from_sitemap(sitemap_index)
print(f"Sous-sitemaps extraits : {sub_sitemaps}")

# Extraction de toutes les URLs d'articles (limite : 8000 articles)
all_articles = []
max_articles = 8000

for sitemap in sub_sitemaps:
    if len(all_articles) >= max_articles:
        break
    extracted_urls = extract_urls_from_sitemap(sitemap)
    all_articles.extend(extracted_urls)

# Limiter la liste des articles √† 8000
all_articles = all_articles[:max_articles]
print(f"Nombre total d'articles trouv√©s : {len(all_articles)}")

if all_articles:
    print("Exemple d'URL :", all_articles[0])
else:
    print("Aucune URL d'article trouv√©e.")

# Connexion √† la collection MongoDB "articles_fr"
collection = get_mongo_collection("articles_fr")


def scrape_article(url):
    """
    R√©cup√®re et enregistre le contenu d'un article √† partir de son URL.
    
    :param url: URL de l'article √† scraper
    """
    headers = {"User-Agent": "Mozilla/5.0"}
    
    try:
        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code != 200:
            print(f"‚ùå Erreur {response.status_code} sur {url}")
            return

        soup = BeautifulSoup(response.text, "html.parser")

        # Extraction du titre de l'article
        titre = soup.find("h1", class_="post-title")
        titre = titre.text.strip() if titre else "Titre inconnu"

        # Extraction de la date de publication
        time_tag = soup.find("time", class_="date-post")
        date = time_tag["datetime"] if time_tag and time_tag.get("datetime") else "Date inconnue"

        # Extraction de l'auteur
        auteur_span = soup.find("span", class_="author")
        auteur = auteur_span.find("a").text.strip() if auteur_span and auteur_span.find("a") else "Auteur inconnu"

        # Extraction du contenu principal
        contenu_div = soup.find("div", class_="article-content")
        contenu = " ".join([p.text.strip() for p in contenu_div.find_all("p")]) if contenu_div else "Contenu non disponible"

        # V√©rification si l'article est incomplet
        if titre == "Titre inconnu":
            print(f"‚ö†Ô∏è Article ignor√© (titre inconnu) : {url}")
            return

        if "inconnu" in [date, auteur, contenu]:
            print(f"‚ö†Ô∏è Article avec informations manquantes : {url}")
            return

        # V√©rifier si l'article est d√©j√† enregistr√© dans MongoDB
        if collection.find_one({"url": url}):
            print(f"‚ö†Ô∏è Article d√©j√† enregistr√© : {url}")
            return

        # Cr√©ation de l'objet article √† ins√©rer dans la base de donn√©es
        article = {
            "url": url,
            "titre": titre,
            "auteur": auteur,
            "contenu": contenu,
            "source": "hespress_fr"
        }

        # Insertion de l'article dans la collection MongoDB
        collection.insert_one(article)
        print(f"‚úÖ Article enregistr√© (Fran√ßais) : {titre}")
    
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping de l'article {url}: {e}")

# Scraper et enregistrer chaque article dans la base de donn√©es
for url in all_articles:
    if collection.find_one({"url": url}):
        print(f"‚ö†Ô∏è Article d√©j√† enregistr√©, pass√© : {url}")
        continue
    scrape_article(url)

print("üìÇ Tous les nouveaux articles sont enregistr√©s dans MongoDB !")
