import time
import random
import requests
from bs4 import BeautifulSoup
import sys
import os
# Ajouter le r√©pertoire racine du projet au chemin d'acc√®s pour les imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))
from config.mongodb import get_mongo_collection  # Import de la fonction pour se connecter √† MongoDB

# Fonction pour extraire les URLs d'un sitemap
def extract_urls_from_sitemap(sitemap_url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    try:
        # Effectuer une requ√™te HTTP GET pour obtenir le contenu du sitemap
        response = requests.get(sitemap_url, headers=headers, timeout=10)
        # Ajouter un d√©lai al√©atoire entre les requ√™tes pour √©viter de surcharger le serveur
        time.sleep(random.uniform(1, 3))

        # Si la r√©ponse n'est pas 200 (OK), afficher un message d'erreur et retourner une liste vide
        if response.status_code != 200:
            print(f"‚ùå Erreur {response.status_code} pour {sitemap_url}")
            return []

        # Utiliser BeautifulSoup pour analyser le contenu XML du sitemap
        soup = BeautifulSoup(response.content, "xml")
        print(f"R√©cup√©ration du sitemap : {sitemap_url}")

        # Extraire toutes les URLs contenues dans les balises <loc>
        urls = [loc.text for loc in soup.find_all("loc")]

        # Filtrer les URLs pour exclure celles se terminant par .jpg ou .jpeg
        filtered_urls = [url for url in urls if not (url.endswith(".jpg") or url.endswith(".jpeg"))]

        # Retourner la liste des URLs filtr√©es
        return filtered_urls

    except Exception as e:
        # G√©rer les erreurs √©ventuelles (ex. erreur r√©seau)
        print(f"‚ùå Erreur lors de l'extraction du sitemap {sitemap_url}: {e}")
        return []

# R√©cup√©rer le sitemap principal
sitemap_index = "https://www.hespress.com/sitemap.xml"
sub_sitemaps = extract_urls_from_sitemap(sitemap_index)

# Afficher les sous-sitemaps extraits
print(f"Sous-sitemaps extraits : {sub_sitemaps}")

# Liste pour stocker toutes les URLs d'articles extraites
all_articles = []
max_articles = 8000  # Limiter le nombre d'articles √† 8000

# Extraire les URLs des articles depuis les sous-sitemaps
for sitemap in sub_sitemaps:
    if len(all_articles) >= max_articles:
        break  # Si le nombre maximum d'articles est atteint, arr√™ter l'extraction
    all_articles.extend(extract_urls_from_sitemap(sitemap))

# Limiter le nombre d'articles √† 8000
all_articles = all_articles[:max_articles]

# Afficher le nombre total d'articles trouv√©s
print(f"Nombre total d'articles trouv√©s : {len(all_articles)}")

# Afficher un exemple d'URL d'article (si des articles ont √©t√© trouv√©s)
if all_articles:
    print("Exemple d'URL :", all_articles[0])
else:
    print("Aucune URL d'article trouv√©e.")

# Connexion √† la collection MongoDB "articles_ar" pour enregistrer les articles
collection = get_mongo_collection("articles_ar")

# Fonction pour scraper les informations d'un article √† partir de son URL
def scrape_article(url):
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        # Effectuer une requ√™te HTTP GET pour r√©cup√©rer la page de l'article
        response = requests.get(url, headers=headers, timeout=10)

        # Si la r√©ponse n'est pas 200 (OK), afficher un message d'erreur
        if response.status_code != 200:
            print(f"‚ùå Erreur {response.status_code} sur {url}")
            return

        # Analyser le contenu HTML de la page de l'article avec BeautifulSoup
        soup = BeautifulSoup(response.text, "html.parser")

        # Extraire le titre de l'article
        titre = soup.find("h1", class_="post-title")
        titre = titre.text.strip() if titre else "Titre inconnu"

        # Extraire la date de publication de l'article
        time_tag = soup.find("time", class_="date-post")
        date = time_tag["datetime"] if time_tag and time_tag.get("datetime") else "Date inconnue"

        # Extraire l'auteur de l'article
        auteur_span = soup.find("span", class_="author")
        auteur = auteur_span.find("a").text.strip() if auteur_span and auteur_span.find("a") else "Auteur inconnu"

        # Extraire le contenu de l'article
        contenu_div = soup.find("div", class_="article-content")
        contenu = " ".join([p.text.strip() for p in contenu_div.find_all("p")]) if contenu_div else "Contenu non disponible"

        # Si le titre de l'article est inconnu, l'ignorer
        if titre == "Titre inconnu":
            print(f"‚ö†Ô∏è Article ignor√© (titre inconnu) : {url}")
            return

        # Si des informations essentielles manquent (date, auteur, contenu), ignorer l'article
        if "inconnu" in [date, auteur, contenu]:
            print(f"‚ö†Ô∏è Article avec informations manquantes : {url}")
            return

        # Si l'article est d√©j√† enregistr√© dans la base de donn√©es, l'ignorer
        if collection.find_one({"url": url}):
            print(f"‚ö†Ô∏è Article d√©j√† enregistr√© : {url}")
            return

        # Cr√©er un dictionnaire avec les informations de l'article
        article = {
            "url": url,
            "titre": titre,
            "auteur": auteur,
            "contenu": contenu,
            "source": "hespress"  # Ajouter la source "hespress"
        }

        # Ins√©rer l'article dans la collection MongoDB
        collection.insert_one(article)
        print(f"‚úÖ Article enregistr√© : {titre}")

    except Exception as e:
        # G√©rer les erreurs √©ventuelles lors du scraping de l'article
        print(f"‚ùå Erreur lors du scraping de l'article {url}: {e}")

# Scraper tous les articles extraits
for url in all_articles:
    # Si l'article est d√©j√† enregistr√©, passer au suivant
    if collection.find_one({"url": url}):
        print(f"‚ö†Ô∏è Article d√©j√† enregistr√©, pass√© : {url}")
        continue  

    # Scraper l'article
    scrape_article(url)

# Afficher un message indiquant que tous les nouveaux articles ont √©t√© enregistr√©s dans MongoDB
print("üìÇ Tous les nouveaux articles sont enregistr√©s dans MongoDB !")
