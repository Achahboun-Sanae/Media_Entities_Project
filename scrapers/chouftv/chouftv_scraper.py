import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
import time
import random
import sys
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urljoin, unquote

# Configuration
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))
from config.mongo_atlass import get_mongo_atlass_collection

<<<<<<< HEAD

collection = get_mongo_atlass_collection("articles_chouftv_new2")

=======
collection = get_mongo_atlass_collection("articles_chouftv_sport")

>>>>>>> 279f7cd5a57f4a08a6efea424b503f95e2d0cd35
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

CATEGORIES = {
<<<<<<< HEAD
    "press": [
        "https://chouftv.ma/press/presscategory/international"
    ],
=======
    '''
    "press": [
        "https://chouftv.ma/press/presscategory/politique",
        "https://chouftv.ma/press/presscategory/societe",
        "https://chouftv.ma/press/presscategory/international"
    ],
    '''
>>>>>>> 279f7cd5a57f4a08a6efea424b503f95e2d0cd35
    "sport": [
        "https://chouftv.ma/sport/sportcategory/ÿØŸàÿ±Ÿäÿßÿ™",
        "https://chouftv.ma/sport/sportcategory/ÿ±Ÿäÿßÿ∂ÿßÿ™-ÿ£ÿÆÿ±Ÿâ",
    ]
}

def parse_arabic_date(date_str):
    """
    Convertit une date arabe comme "ÿßŸÑÿ£ÿ≠ÿØ 20 ÿ£ÿ®ÿ±ŸäŸÑ 2025 | 18:41" ou "ÿßŸÑÿ£ÿ≠ÿØ 20 ÿ£ÿ®ÿ±ŸäŸÑ 202518:41"
    en objet datetime. Retourne None si la conversion √©choue.
    """
    if not date_str:
        return None

    # Dictionnaire de traduction des mois arabes
    month_trans = {
        'ŸäŸÜÿßŸäÿ±': 'January', 'ŸÅÿ®ÿ±ÿßŸäÿ±': 'February', 'ŸÖÿßÿ±ÿ≥': 'March',
        'ÿ£ÿ®ÿ±ŸäŸÑ': 'April', 'ŸÖÿßŸä': 'May', 'ŸäŸàŸÜŸäŸà': 'June',
        'ŸäŸàŸÑŸäŸà': 'July', 'ÿ∫ÿ¥ÿ™': 'August', 'ÿ¥ÿ™ŸÜÿ®ÿ±': 'September',
        'ÿ£ŸÉÿ™Ÿàÿ®ÿ±': 'October', 'ŸÜŸàŸÜÿ®ÿ±': 'November', 'ÿØÿ¨ŸÜÿ®ÿ±': 'December'
    }

    try:
        # Nettoyer la cha√Æne
        date_str = date_str.strip()
        
        # S√©parer la date et l'heure
        if '|' in date_str:
            # Format avec s√©parateur: "ÿßŸÑÿ£ÿ≠ÿØ 20 ÿ£ÿ®ÿ±ŸäŸÑ 2025 | 20:47"
            date_part, time_part = [p.strip() for p in date_str.split('|')]
        else:
            # Format sans s√©parateur: "ÿßŸÑÿ£ÿ≠ÿØ 20 ÿ£ÿ®ÿ±ŸäŸÑ 202520:47"
            # Trouver o√π commence l'heure (recherche du premier ':' pr√©c√©d√© de 2 chiffres)
            hour_index = -1
            for i in range(len(date_str)):
                if date_str[i] == ':' and i >= 2 and date_str[i-2:i].isdigit():
                    hour_index = i - 2
                    break
            
            if hour_index > 0:
                date_part = date_str[:hour_index].strip()
                time_part = date_str[hour_index:].strip()
            else:
                date_part = date_str
                time_part = "00:00"

        # Extraire les composants de la date
        date_parts = date_part.split()
        if len(date_parts) < 3:
            return None

        day = date_parts[-3]  # 20
        month_ar = date_parts[-2]  # ÿ£ÿ®ÿ±ŸäŸÑ
        year = date_parts[-1]  # 2025

        # Traduire le mois
        month_en = month_trans.get(month_ar)
        if not month_en:
            return None

        # Traiter l'heure
        try:
            if ':' in time_part:
                hours, minutes = map(int, time_part.split(':'))
            else:
                # Si l'heure est mal format√©e (ex: "2047" au lieu de "20:47")
                if len(time_part) >= 4 and time_part[:2].isdigit() and time_part[2:4].isdigit():
                    hours = int(time_part[:2])
                    minutes = int(time_part[2:4])
                else:
                    hours, minutes = 0, 0
        except:
            hours, minutes = 0, 0

        # Cr√©er l'objet datetime
        date_obj = datetime.strptime(f"{day} {month_en} {year}", "%d %B %Y")
        date_obj = date_obj.replace(hour=hours, minute=minutes)

        return date_obj

    except Exception as e:
        print(f"Erreur de conversion de la date '{date_str}': {str(e)}")
        return None

<<<<<<< HEAD
def get_article_urls(max_articles_per_category=7000):
=======
def get_article_urls(max_articles_per_category=1700):
>>>>>>> 279f7cd5a57f4a08a6efea424b503f95e2d0cd35
    """R√©cup√®re les URLs des articles"""
    article_urls = set()
    
    for section, categories in CATEGORIES.items():
        for category_url in categories:
            print(f"üîç Scanning category: {unquote(category_url)}")
            page = 1
            category_articles = 0
            
            while category_articles < max_articles_per_category:
                try:
                    if page > 1:
                        category_page_url = f"{category_url}/page/{page}"
                    else:
                        category_page_url = category_url
                    
                    response = requests.get(category_page_url, headers=HEADERS, timeout=10)
                    if response.status_code != 200:
                        if response.status_code == 404:
                            print(f"‚ö†Ô∏è Category not found: {unquote(category_url)}")
                        else:
                            print(f"‚ùå Error {response.status_code} on {unquote(category_page_url)}")
                        break

                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    articles = soup.find_all("a", href=True)
                    found_articles = 0
                    
                    for link in articles:
                        href = link["href"]
                        if f"/{section}/" in href and href not in article_urls:
                            full_url = urljoin(category_url, href)
                            article_urls.add(full_url)
                            category_articles += 1
                            found_articles += 1
                            if category_articles >= max_articles_per_category:
                                break
                    
                    if found_articles == 0:
                        break
                    
                    print(f"üìÑ Page {page}: Found {found_articles} articles (Total: {category_articles})")
                    page += 1
                    time.sleep(random.uniform(1, 3))
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing {unquote(category_url)}: {str(e)}")
                    break
    
    return list(article_urls)

def scrape_article(url):
    """Scrape un article avec gestion am√©lior√©e"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        
        # Titre
        title_tag = soup.find("h1")
        title = title_tag.get_text(strip=True) if title_tag else "ÿπŸÜŸàÿßŸÜ ÿ∫Ÿäÿ± ŸÖÿπÿ±ŸàŸÅ"
        
        # Contenu
        content_div = soup.find("div", class_="middleContent")
        content = ""
        if content_div:
            paragraphs = [p.get_text(strip=True) for p in content_div.find_all("p") 
                        if not p.find_parent("li", class_="pub300-250")]
            content = " ".join(paragraphs)
        
        if not content:
            return None
        
        # Date de publication
        published_date = None
        time_tag = soup.find('time')
        if time_tag:
            date_text = time_tag.get_text(strip=True)
            published_date = parse_arabic_date(date_text)
        
        # Auteur
        author = "ÿ¥ŸàŸÅ ÿ™Ÿä ŸÅŸä"
        source_span = soup.find("span", class_="source")
        if source_span:
            author = source_span.get_text(strip=True).replace("ÿßŸÑŸÖÿµÿØÿ±:", "").strip()
        
        # Cat√©gorie
        category = "ÿ∫Ÿäÿ± ŸÖÿπÿ±ŸàŸÅ"
        navbar = soup.find("ul", class_="navbar-head")
        if navbar:
            breadcrumb_links = navbar.find_all("a", href=True)
            if len(breadcrumb_links) > 1:
                category = breadcrumb_links[-1].get_text(strip=True)
        
        # Fallback par URL
        if category == "ÿ∫Ÿäÿ± ŸÖÿπÿ±ŸàŸÅ":
            for cat_ar, cat_urls in CATEGORIES.items():
                if any(cat_url in url for cat_url in cat_urls):
                    category = cat_ar
                    break
        
        # V√©rification des doublons
        if collection.find_one({"url": url}):
            return None
        
        return {
            "url": url,
            "titre": title,
            "auteur": author,
            "date": published_date,
            "categorie": category,
            "contenu": content,
<<<<<<< HEAD
            "source": "Chouf TV",
=======
            "source": "ÿ¥ŸàŸÅ ÿ™Ÿä ŸÅŸä",
>>>>>>> 279f7cd5a57f4a08a6efea424b503f95e2d0cd35
            "date_import": datetime.now()
        }
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error scraping {url}: {str(e)}")
        return None

def main():
    """Fonction principale"""
    print("üöÄ Starting ChoufTV scraper...")
    
    article_urls = get_article_urls()
    print(f"‚úÖ Found {len(article_urls)} articles. Starting scraping...")
    
    successful = 0
    with ThreadPoolExecutor(max_workers=5) as executor:
        for i, result in enumerate(executor.map(scrape_article, article_urls)):
            if result:
                collection.insert_one(result)
                successful += 1
            print(f"üìä Progress: {i+1}/{len(article_urls)} articles processed", end="\r")
    
    print(f"\nüíæ Saved {successful} articles to MongoDB.")
    print("‚úÖ All done!")

if __name__ == "__main__":
    main()