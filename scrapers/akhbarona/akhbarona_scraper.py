import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
import time
import random

# Configuration MongoDB
MONGO_URI = "mongodb://localhost:27017/"
DATABASE_NAME = "medias_maroc"
COLLECTION_NAME = "articles"

# Connexion Ã  MongoDB
client = MongoClient(MONGO_URI)
db = client[DATABASE_NAME]
collection = db[COLLECTION_NAME]

# Headers pour Ã©viter d'Ãªtre bloquÃ©
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# CatÃ©gories Ã  scraper
CATEGORIES = ["national"]

# Fonction pour rÃ©cupÃ©rer les URLs des articles de toutes les pages
def get_article_urls():
    base_url = "https://www.akhbarona.com"
    article_urls = set()
    
    for category in CATEGORIES:
        page = 1
        
        while True:  # Continue jusqu'Ã  ce que la page soit introuvable
            url = f"{base_url}/{category}/index.{page}.html"
            print(f"ğŸ” Scraping {url}...")

            response = requests.get(url, headers=HEADERS)

            if response.status_code == 404:
                print(f"âš ï¸ Page {page} pour {category.upper()} introuvable (404), arrÃªt.")
                break  # ArrÃªter cette catÃ©gorie

            if response.status_code != 200:
                print(f"âŒ Erreur {response.status_code} sur {url}, on passe Ã  la suivante.")
                break

            soup = BeautifulSoup(response.text, "html.parser")

            # Trouver tous les liens des articles
            articles = soup.find_all("a", href=True)
            found = 0  # Compteur d'articles trouvÃ©s

            for article in articles:
                href = article["href"]

                # VÃ©rifier si le lien correspond Ã  un article valide
                if "/articles/" in href or any(cat in href for cat in CATEGORIES):
                    full_url = "https://www.akhbarona.com" + href if href.startswith("/") else href
                    if full_url not in article_urls:
                        article_urls.add(full_url)
                        found += 1

            print(f"ğŸ“„ {category.upper()} - Page {page} scannÃ©e, {found} nouveaux articles trouvÃ©s. Total: {len(article_urls)} articles.")

            # Si aucune nouvelle URL trouvÃ©e, c'est peut-Ãªtre la fin de la catÃ©gorie
            if found == 0:
                print(f"ğŸš« Aucune nouvelle URL trouvÃ©e sur {category.upper()} page {page}, arrÃªt de la catÃ©gorie.")
                break

            page += 1
            time.sleep(random.uniform(1, 3))  # Pause alÃ©atoire pour Ã©viter de se faire bloquer

    return list(article_urls)

# Fonction pour scraper un article
def scrape_article(url):
    try:
        response = requests.get(url, headers=HEADERS)

        if response.status_code == 404:
            print(f"âš  Article introuvable (404) : {url}")
            return None

        if response.status_code != 200:
            print(f"âŒ Erreur {response.status_code} sur {url}")
            return None

        soup = BeautifulSoup(response.text, "html.parser")

        # Titre
        titre_tag = soup.find("h1", class_="artical-content-heads")
        titre = titre_tag.text.strip() if titre_tag else "Titre inconnu"

        # Contenu
        contenu_div = soup.find("div", class_="bodystr")
        contenu = " ".join([p.text.strip() for p in contenu_div.find_all("p")]) if contenu_div else "Contenu non disponible"

        # VÃ©rifier si l'article est dÃ©jÃ  en base de donnÃ©es
        if collection.find_one({"url": url}):
            print(f"âš  Article dÃ©jÃ  enregistrÃ© : {titre}")
            return None

        return {
            "url": url,
            "titre": titre,
            "auteur": "Akhbarona",
            "contenu": contenu,
            "source": "Akhbarona",
        }

    except Exception as e:
        print(f"âš  Erreur lors du scraping de {url}: {e}")
        return None


# RÃ©cupÃ©rer les URLs des articles
article_urls = get_article_urls()
print(f"âœ… {len(article_urls)} articles trouvÃ©s. DÃ©but du scraping...")

# Scraper chaque article
articles = []
for i, url in enumerate(article_urls, 1):
    article = scrape_article(url)
    if article:
        articles.append(article)

    # InsÃ©rer dans MongoDB par lots de 100
    if len(articles) >= 100:
        collection.insert_many(articles)
        print(f"ğŸ’¾ {len(articles)} articles enregistrÃ©s dans MongoDB.")
        articles = []  # RÃ©initialiser la liste

    # Pause alÃ©atoire pour Ã©viter de se faire bloquer
    time.sleep(random.uniform(1, 3))

# InsÃ©rer les derniers articles restants
if articles:
    collection.insert_many(articles)
    print(f"ğŸ’¾ {len(articles)} derniers articles enregistrÃ©s dans MongoDB.")

print("âœ… ğŸ“‚ Tous les articles sont enregistrÃ©s !")
