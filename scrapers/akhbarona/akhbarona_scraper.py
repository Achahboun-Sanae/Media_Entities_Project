import requests
from bs4 import BeautifulSoup
import time
import random
import sys
import os

# Ajouter le rÃ©pertoire racine du projet au chemin d'accÃ¨s pour les imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from config.mongodb import get_mongo_collection  # Importer la configuration MongoDB

# Connexion Ã  la collection MongoDB "articles_ar"
collection = get_mongo_collection("articles_ar")

# DÃ©finition des headers pour simuler un vrai navigateur et Ã©viter les blocages
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# Liste des catÃ©gories d'articles Ã  scraper
<<<<<<< HEAD
CATEGORIES = ["culture"]
=======
CATEGORIES = ["culture","Economy","national","world"]
>>>>>>> cb2b3da0d491cb7db20d66dc0cc93e9e30415205

# Fonction pour rÃ©cupÃ©rer les URLs des articles Ã  partir des pages de catÃ©gories
def get_article_urls():
    base_url = "https://www.akhbarona.com"
    article_urls = set()
    
    for category in CATEGORIES:
        page = 1  # Initialisation du compteur de pages
        
        while True:  # Boucle infinie, s'arrÃªtera si une page est introuvable
            url = f"{base_url}/{category}/index.{page}.html"
            print(f"ðŸ” Scraping {url}...")

            response = requests.get(url, headers=HEADERS)

            if response.status_code == 404:  # Si la page n'existe pas, arrÃªter le scraping de cette catÃ©gorie
                print(f"âš ï¸ Page {page} pour {category.upper()} introuvable (404), arrÃªt.")
                break

            if response.status_code != 200:  # Si une autre erreur HTTP survient
                print(f"âŒ Erreur {response.status_code} sur {url}, on passe Ã  la suivante.")
                break

            soup = BeautifulSoup(response.text, "html.parser")

            # Extraction des liens d'articles
            articles = soup.find_all("a", href=True)
            found = 0  # Compteur d'articles trouvÃ©s

            for article in articles:
                href = article["href"]

                # VÃ©rification si l'URL correspond bien Ã  un article
                if "/articles/" in href or any(cat in href for cat in CATEGORIES):
                    full_url = "https://www.akhbarona.com" + href if href.startswith("/") else href
                    if full_url not in article_urls:
                        article_urls.add(full_url)
                        found += 1

            print(f"ðŸ“„ {category.upper()} - Page {page} scannÃ©e, {found} nouveaux articles trouvÃ©s. Total: {len(article_urls)} articles.")

            if found == 0:  # Si aucune nouvelle URL trouvÃ©e, on suppose la fin des articles
                print(f"ðŸš« Aucune nouvelle URL trouvÃ©e sur {category.upper()} page {page}, arrÃªt de la catÃ©gorie.")
                break

            page += 1  # Passer Ã  la page suivante
            time.sleep(random.uniform(1, 3))  # Pause alÃ©atoire pour Ã©viter le blocage du site

    return list(article_urls)

# Fonction pour scraper le contenu d'un article
def scrape_article(url):
    try:
        response = requests.get(url, headers=HEADERS)

        if response.status_code == 404:
            print(f"âš  Article introuvable (404) : {url}")
            return None

        if response.status_code != 200:
            print(f"âŒ Erreur {response.status_code} sur {url}")
            return None

        soup = BeautifulSoup(response.text, "html.parser")

        # Extraction du titre de l'article
        titre_tag = soup.find("h1", class_="artical-content-heads")
        titre = titre_tag.text.strip() if titre_tag else "Titre inconnu"

        # Extraction du contenu de l'article
        contenu_div = soup.find("div", class_="bodystr")
        contenu = " ".join([p.text.strip() for p in contenu_div.find_all("p")]) if contenu_div else "Contenu non disponible"

        # VÃ©rification si l'article est dÃ©jÃ  enregistrÃ© dans la base de donnÃ©es
        if collection.find_one({"url": url}):
            print(f"âš  Article dÃ©jÃ  enregistrÃ© : {titre}")
            return None

        return {
            "url": url,
            "titre": titre,
            "auteur": "Akhbarona",
            "contenu": contenu,
            "source": "Akhbarona",
        }

    except Exception as e:
        print(f"âš  Erreur lors du scraping de {url}: {e}")
        return None

# RÃ©cupÃ©ration des URLs des articles
article_urls = get_article_urls()
print(f"âœ… {len(article_urls)} articles trouvÃ©s. DÃ©but du scraping...")

# Scraper et stocker les articles dans MongoDB
articles = []
for i, url in enumerate(article_urls, 1):
    article = scrape_article(url)
    if article:
        articles.append(article)

    # InsÃ©rer les articles en lots de 100 pour optimiser l'accÃ¨s Ã  la base de donnÃ©es
    if len(articles) >= 100:
        collection.insert_many(articles)
        print(f"ðŸ’¾ {len(articles)} articles enregistrÃ©s dans MongoDB.")
        articles = []  # RÃ©initialiser la liste temporaire

    # Pause alÃ©atoire entre les requÃªtes pour Ã©viter d'Ãªtre dÃ©tectÃ© comme bot
    time.sleep(random.uniform(1, 3))

# InsÃ©rer les derniers articles restants dans MongoDB
if articles:
    collection.insert_many(articles)
    print(f"ðŸ’¾ {len(articles)} derniers articles enregistrÃ©s dans MongoDB.")

print("âœ… ðŸ“‚ Tous les articles sont enregistrÃ©s !")
